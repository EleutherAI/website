---
title: "Generating text using natural language to simulate model's activations"
date: 2024-10-25T16:00:00-00:00
description: "Using interpretations of SAE latents to do inference on the host language model."
author: ["Gon√ßalo Paulo", "Nora Belrose"]
ShowToc: true
mathjax: true
draft: false
---

# Generating text by simulating the model using natural language


![Example of the interactive demo](/static/images/blog/generating-text-using-nl-to-simulate-activations/image.png)
*This interactive demo would use a sentence, e.g. "The sun was", which was completed by repeated generationg of a LLM. There are two lines in the first panel, one for the words that are generated by the "unaltered" model, and one for the words that are generated by the "altered" model, at different fractions of correct latents. Depending on the slider we would have different fractions of correct activations, which would be shown with some mark on the second panel. 
By clicking on the words we would see the correct latent activations for each word, sorted by their value, and the corresponding interpretations. Because there are 40 or so active latents, there would be a slider.
By clicking on the latent number we would see the "dashboard" of the different activations of that latent, with the activation values of the different words.*


It could be argued that one of the greatest possible interpretability results in LLMs would be the ability to reproduce the behaviour of the model using natural language. How far away are we from this goal if we use Sparse Autoencoders (SAEs) and their interpretations to simulate the activations of a language model?

(High level description of SAEs, their limitations and how we generated the explanations)

There are 3 main problems to be adressed with this approach. First one needs to correctly identify which latents are active. Secondly, the value of the activation for each of the active latents needs to be recovered from the explanation. Thirdly, because there are orders of magnitude more non-active features than active ones, the rate of false positives needs to be very low.

**Key results**
- Current SAE interpretations can only get the 50\% of active latents correct in arbitrary contexts. Despite this, for a single layer "substitution" this is enough to generate "coherent" text.
- (We still have not done this but should) We find that explanations can/can't be used to effectively simulate the value of the activations of the model when calibrated.
- Although explanations correctly identify 90% of non-active latents, a value closer to 99.9% is needed to generate "coherent" text.
- (We still have not done this but should) We find that the pre-generated scores for the explanations are/are not predictive of how frequently they are identified by the model, and find that the scores can/can't be used by the model to calibrate its predictions.


# How many latents are needed to recover the model behaviour?

- At the moment we know that SAEs don't fully recover model's capabilities (Leo shows that adding a single layer is a drop of 10% of training compute), but we are still going to take this value as the skyline.
- If we only use the top 50\% of the top activating latents, we get a KL divergence that is on the same order of magnitude as the one we get when we use all the latents, and we recover X\% of the CE loss. This is for a single layer "substitution".
- (Do we want to do some question of a benchmark to give "benchmark" performance?)
- (Do we want to do a full layer substitution? I think showing that one layer already is this bad is enough. We could just say that each subsequent layer substitution would make the performance drop even more.)
- We use the correct activation value here. What if we sample the activation value from it's distribution?

![performace recovered](/static/images/blog/generating-text-using-nl-to-simulate-activations/image-1.png)

# Using latent interpretations to predict if they are active.

- Current explanations correctly identify 90\% of non-active latents, and only 50\% of active latents. (Top vs quantile explanations comparison)
- Finetuning a model to predict if a latent is active based on the explanation did not significantly improve the results.
- If "cheat" and ask the model only about the active latents, use the ones the model gets right, and then fill the rest with latents the model gets wrong, the text is "coherent".
- This means that even before thinking about precision, this task fails when it can't correctly identify the active latents.
- (We still have not done this but should) Scores can/can't be used to predict if a model will get a latent right.
- (Work more on this) Using contexts instead of explanations didn't work out

![correctly identified latents](/static/images/blog/generating-text-using-nl-to-simulate-activations/image-2.png)
![scores predictiveness](/static/images/blog/generating-text-using-nl-to-simulate-activations/image-3.png)


# (TODO) Predicting the activation value with latent interpretations.

- We already know that simulation sucks and that it gives bad results. 
- (Have the model predict the activation quantile instead of the value.)
- (Have the model decide between different quantiles)
- (Few shot examples of activations from that latent)

![activation value prediction](/static/images/blog/generating-text-using-nl-to-simulate-activations/image-4.png)


# Correctly identifying non-active latents.

- The model correctly identifies 90\% of non-active latents. But we need to identify 99.9\% of them to generate "coherent" text.
- (Use the score of latents to calibrate the model)



![false positives](/static/images/blog/generating-text-using-nl-to-simulate-activations/image-5.png)